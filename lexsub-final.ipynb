{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lexsub: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexsub import *\n",
    "import os\n",
    "import re\n",
    "\n",
    "from copy import deepcopy\n",
    "from pymagnitude import *\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "from lexsub_check import precision\n",
    "\n",
    "DATA = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target:side substitutes:sides edge bottom front club line both back place corner\n",
      "target:side substitutes:sides edge bottom front club line both back place corner\n",
      "target:side substitutes:sides edge bottom front club line both back place corner\n",
      "target:side substitutes:sides edge bottom front club line both back place corner\n",
      "target:side substitutes:sides edge bottom front club line both back place corner\n",
      "target:side substitutes:sides edge bottom front club line both back place corner\n",
      "target:side substitutes:sides edge bottom front club line both back place corner\n",
      "target:side substitutes:sides edge bottom front club line both back place corner\n",
      "target:side substitutes:sides edge bottom front club line both back place corner\n",
      "target:side substitutes:sides edge bottom front club line both back place corner\n",
      "target:told substitutes:reporters asked said interview saying afp quoted insisted telling spoke\n",
      "target:tell substitutes:know why ask you me do let sure telling ca\n",
      "target:tell substitutes:know why ask you me do let sure telling ca\n",
      "target:tell substitutes:know why ask you me do let sure telling ca\n",
      "target:told substitutes:reporters asked said interview saying afp quoted insisted telling spoke\n",
      "target:tell substitutes:know why ask you me do let sure telling ca\n",
      "target:told substitutes:reporters asked said interview saying afp quoted insisted telling spoke\n",
      "target:told substitutes:reporters asked said interview saying afp quoted insisted telling spoke\n",
      "target:tell substitutes:know why ask you me do let sure telling ca\n",
      "target:tells substitutes:asks informs explains finds reveals realizes learns discovers reminds tell\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join(DATA,'glove.6B.100d.magnitude'))\n",
    "output = []\n",
    "keys = []\n",
    "with open(os.path.join(DATA,'input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        keyIndex = int(fields[0].strip())\n",
    "        sentence = fields[1].strip().split()\n",
    "        keys.append(sentence[keyIndex])\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), sentence)))\n",
    "\n",
    "for i, syn in enumerate(output[:20]):\n",
    "    print(\"target:%s\" % keys[i], \"substitutes:%s\" % output[i])\n",
    "#     print(\"\\n\".join(output[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=27.89\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join(DATA,'reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Write some beautiful documentation of your program here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 0.87980753),\n",
       " ('rabbit', 0.7424427),\n",
       " ('cats', 0.7323004),\n",
       " ('monkey', 0.72887105),\n",
       " ('pet', 0.719014)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv = Magnitude(DATA + \"/glove.6B.100d.magnitude\")\n",
    "len(wv) # how many words in this word vector file\n",
    "wv.dim # the dimensionality of each word vector\n",
    "wv.most_similar(\"cat\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog [ 0.0546582  0.0548728  0.0936535 -0.1641379 -0.1306658]\n",
      "cat [ 0.0458157  0.0561247  0.1253741 -0.1178949 -0.1162836]\n"
     ]
    }
   ],
   "source": [
    "for key, vector in wv:\n",
    "    if key == \"cat\" or key == \"dog\":\n",
    "        print(key, vector[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dog and cat vectors are similar and the reason why we saw that result above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create $ \\hat{Q} $ and it's corresponding vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = Magnitude(DATA + \"/glove.6B.100d.magnitude\")\n",
    "vocabQHat = set([k for k, v in wv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to load lexicons and copy pymagnitude vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNumber = re.compile(r'\\d+.*')\n",
    "\n",
    "def norm_word(word):\n",
    "    if isNumber.search(word.lower()):\n",
    "        return '---num---'\n",
    "    elif re.sub(r'\\W+', '', word) == '':\n",
    "        return '---punc---'\n",
    "    else:\n",
    "        return word.lower()\n",
    "\n",
    "\n",
    "def build_lexicon(filename):\n",
    "    lexicon = {}\n",
    "    for line in open(filename, 'r'):\n",
    "        words = line.lower().strip().split()\n",
    "        lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]\n",
    "    return lexicon\n",
    "\n",
    "\n",
    "def copyPymagnitude(wv):\n",
    "    return deepcopy({k: v for k, v in wv})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_substitutability(t, s):\n",
    "        score = wv.similarity(t, s)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 10\n",
    "\n",
    "qHat = copyPymagnitude(wv)\n",
    "q = copyPymagnitude(wv)\n",
    "lexicon = build_lexicon(os.path.join(DATA, \"lexicons\", \"wordnet-synonyms+.txt\"))\n",
    "loopOn = vocabQHat.intersection(set(lexicon.keys()))\n",
    "\n",
    "for t in range(T):\n",
    "    for word in loopOn:\n",
    "        wordNeighbours = set(lexicon[word]).intersection(vocabQHat)\n",
    "        cand_scores = []\n",
    "        for x in wordNeighbours:\n",
    "            cand_scores.append(get_substitutability(x, word))\n",
    "            \n",
    "        sorted_candidates = sorted(zip(list(wordNeighbours), cand_scores), key = lambda x : x[1], reverse=True )            \n",
    "        wordNeighbours = [sub for sub, score in sorted_candidates][:10]\n",
    "        \n",
    "        #if len(wordNeighbours)<10:\n",
    "            #numNeighbour = len(wordNeighbours)\n",
    "        \n",
    "        numNeighbours = len(wordNeighbours)\n",
    "        \n",
    "        if numNeighbours == 0: \n",
    "            continue\n",
    "            \n",
    "        newVec = numNeighbours * qHat[word]\n",
    "        \n",
    "        for ppWord in wordNeighbours:\n",
    "            newVec += q[ppWord]\n",
    "            \n",
    "        q[word] = newVec / (2*numNeighbours)\n",
    "        \n",
    "# Q is ready for writing at this point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Q to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA + \"/glove.6B.100d.retrofit.large.context.10.txt\", \"w\") as f:\n",
    "    for i, (k, v) in enumerate(q.items()):\n",
    "        line = \" \".join([k] + list(map(str, v)) + [\"\\n\"])\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate again using retrofit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "front bottom back edge line corner left place right way\n",
      "front bottom back edge line corner left place right way\n",
      "front bottom back edge line corner left place right way\n",
      "front bottom back edge line corner left place right way\n",
      "front bottom back edge line corner left place right way\n",
      "front bottom back edge line corner left place right way\n",
      "front bottom back edge line corner left place right way\n",
      "front bottom back edge line corner left place right way\n",
      "front bottom back edge line corner left place right way\n",
      "front bottom back edge line corner left place right way\n",
      "Score=43.57\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join(DATA,'glove.6B.100d.retrofit.fin.10.magnitude'))\n",
    "output = []\n",
    "\n",
    "# Run on the retrofit file\n",
    "with open(os.path.join(DATA,'input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))\n",
    "\n",
    "# Check score\n",
    "with open(os.path.join(DATA,'reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set(nltk_stopwords.words('english')))\n",
    "\n",
    "def get_words(s, index, window=5):\n",
    "    \"\"\" Extract a list of words from a sentence string with punctuation, spaces etc \n",
    "    s = sentence \n",
    "    \"\"\"\n",
    "    # strip punctuation \n",
    "    s = re.sub(r'[^\\w\\s]','',s)\n",
    "    # replace newline \n",
    "    s = s.replace('\\n', ' ')\n",
    "    # get rid of spaces\n",
    "    s = \" \".join(s.split())\n",
    "    list_sentence = s.split(' ')\n",
    "\n",
    "    mid_window = window // 2\n",
    "\n",
    "    if index > mid_window:\n",
    "        l = index - mid_window\n",
    "    else:\n",
    "        l = 0\n",
    "\n",
    "    r = index + mid_window\n",
    "\n",
    "    return list_sentence[l:r+1]\n",
    "\n",
    "def unique(iter):\n",
    "    \"removes duplicates from iterable preserving order\"\n",
    "    result = list()\n",
    "    seen = set()\n",
    "    for x in iter:\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            result.append(x)\n",
    "    return result\n",
    "\n",
    "def process_candidates(candidates, target):\n",
    "    \"\"\" words to lower case, replace underscores, remove duplicated words, \n",
    "        filter out target word and stop words \"\"\"\n",
    "    filterwords = stopwords + [target]\n",
    "    return unique(filter(lambda x : x not in filterwords, \n",
    "                  map(lambda s : s.lower().replace('_', ' '), candidates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexSub:\n",
    "\n",
    "    def __init__(self, wvec_file, topn=100):\n",
    "        self.wvecs = pymagnitude.Magnitude(wvec_file)\n",
    "        self.n_candidates = 100\n",
    "        self.n_substitutes = 10\n",
    "\n",
    "    def substitutes(self, index, sentence):\n",
    "        \"Return ten guesses that are appropriate lexical substitutions for the word at sentence[index].\"\n",
    "        sentence_list = sentence.split(\" \")\n",
    "        #return(list(map(lambda k: k[0], self.wvecs.most_similar(sentence_list[index], topn=self.topn))))\n",
    "        word = sentence_list[index] \n",
    "        words_scores = self.wvecs.most_similar(positive=[word])\n",
    "        result = [word for word, score in words_scores]\n",
    "        result = process_candidates(result, word)[:self.n_candidates]\n",
    "        return result\n",
    "        \n",
    "    def get_substitutability(self, t, s, C):\n",
    "        \"\"\" get substitutability of substitution s for target t in context C\n",
    "        t = target word \n",
    "        s = candidate substitution \n",
    "        C = list of context words \n",
    "        \"\"\"\n",
    "        # 1. target score: how similar is it to the target word?\n",
    "        tscore = self.wvecs.similarity(t, s)\n",
    "        # 2. context score: how similar is it to the context words?\n",
    "        cscores = [self.wvecs.similarity(s, c) for c in C ]\n",
    "        cscore = sum(cscores)\n",
    "        return (len(C)*tscore + cscore)/(2*len(C) + 1)  # Add +1 in denom to avoid div by zero error\n",
    "\n",
    "    def lex_sub(self, index, sentence):\n",
    "        \"\"\" Get appropriate substitution for a word given context words \n",
    "        \n",
    "        word_POS = word with part of speech in form word.POS e.g. dog.n\n",
    "        context_words = list of words in context \n",
    "        \"\"\"\n",
    "        list_ = sentence.split(\" \")\n",
    "        w = list_[index]\n",
    "        #w,_,POS = word_POS.partition('.')\n",
    "        # generate candidate substitutions\n",
    "        candidates = self.substitutes(index, sentence)\n",
    "        if sentence is None:\n",
    "            return candidates[:self.n_substitutes]\n",
    "        else:\n",
    "            context_words = get_words(sentence, index=index, window=5)\n",
    "            # filter context words: exist in the word2vec vocab, not stop words  \n",
    "            #context_words = list(filter(lambda c : c in vocabQHat \n",
    "                                              # and c not in stopwords, \n",
    "                                              # context_words))\n",
    "                    \n",
    "            context_words = list(filter(lambda c : c in self.wvecs \n",
    "                                              and c not in stopwords and c != w, \n",
    "                                              context_words))\n",
    "            \n",
    "            cand_scores = [self.get_substitutability(w, s, context_words) if s in self.wvecs else 0 for s in candidates ]\n",
    "            assert(len(cand_scores) == len(candidates))            \n",
    "            sorted_candidates = sorted(zip(candidates, cand_scores), key = lambda x : x[1], reverse=True )\n",
    "            return [sub for sub, score in sorted_candidates][:self.n_substitutes]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target:side substitutes:bottom corner back place left edge front line right way\n",
      "target:side substitutes:way back place right front line bottom edge left corner\n",
      "target:side substitutes:back way right place front line left edge bottom corner\n",
      "target:side substitutes:front bottom back edge line corner left place right way\n",
      "target:side substitutes:way front right back place left corner line bottom edge\n",
      "target:side substitutes:front bottom back edge line corner left place right way\n",
      "target:side substitutes:place right way back front bottom left line corner edge\n",
      "target:side substitutes:front bottom back edge line corner left place right way\n",
      "target:side substitutes:way front line back right place bottom edge left corner\n",
      "target:side substitutes:line corner front edge place back way left bottom right\n",
      "target:told substitutes:reporters asked interview afp quoted insisted spokesman press statement spoke\n",
      "target:tell substitutes:say know answer ask let think remember hear\n",
      "target:tell substitutes:know ask remember hear let say think answer\n",
      "target:tell substitutes:know ask remember hear let say think answer\n",
      "target:told substitutes:asked interview press reporters statement insisted spoke afp spokesman quoted\n",
      "target:tell substitutes:know let answer say think remember hear ask\n",
      "target:told substitutes:asked reporters interview press insisted statement spoke spokesman afp quoted\n",
      "target:told substitutes:reporters asked interview afp quoted insisted spokesman press statement spoke\n",
      "target:tell substitutes:know ask remember hear let say think answer\n",
      "target:tells substitutes:explains reveals finds asks discovers learns reminds informs realizes convinces\n",
      "Score=41.51\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join(DATA,'glove.6B.100d.retrofit.fin.10.magnitude'))\n",
    "output = []\n",
    "keys = []\n",
    "\n",
    "# Run on the retrofit file\n",
    "with open(os.path.join(DATA,'input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        index = int(fields[0].strip())\n",
    "        list_sentence = fields[1].strip().split(\" \")\n",
    "        str_sentence = fields[1].strip()\n",
    "        \n",
    "        keys.append(list_sentence[index])\n",
    "        \n",
    "        \n",
    "        output.append(\" \".join(lexsub.lex_sub(int(fields[0]), str_sentence)))\n",
    "        \n",
    "for i, syn in enumerate(output[:20]):\n",
    "    print(\"target:%s\" % keys[i], \"substitutes:%s\" % output[i])\n",
    "\n",
    "# Check score\n",
    "with open(os.path.join(DATA,'reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'a', 'guy', 'who', 'is', 'learning', 'nlp']\n",
      "['a', 'guy', 'who', 'is', 'learning']\n"
     ]
    }
   ],
   "source": [
    "test = \"I am a guy who is learning nlp\".split()\n",
    "print(test)\n",
    "index = 4 # who\n",
    "window = 5\n",
    "\n",
    "mid_window = window // 2\n",
    "\n",
    "if index > mid_window:\n",
    "    l = index - mid_window\n",
    "else:\n",
    "    l = 0\n",
    "\n",
    "r = index + mid_window\n",
    "\n",
    "print(test[l:r+1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
