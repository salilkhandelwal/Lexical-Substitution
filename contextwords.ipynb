{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pymagnitude'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-83ac3a0b234a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mDATA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpymagnitude\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pymagnitude'"
     ]
    }
   ],
   "source": [
    "DATA = \"../data\"\n",
    "from pymagnitude import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "stopwords = [\"it's\", \"she's\", 'were', 'because', 'this', 'couldn', 'then', 'how'\n",
    ", 'd', 'doesn', 'down', 's', 'they', 'she', \"needn't\", 'wasn', 'haven', \n",
    "'between', \"wouldn't\", 'the', 'ma', \"wasn't\", 'until', 'my', 'himself', \n",
    "\"that'll\", 'by', 'about', 'in', \"aren't\", \"should've\", 'why', 'nor', \n",
    "'before', 'when', 'we', 'here', 'only', \"couldn't\", 'ain', 'no', 'your', \n",
    "'will', 'own', 'his', \"you'll\", 'are', 'and', 'most', 'do', 'now', \"isn't\", \n",
    "'having', 'on', 'her', 'theirs', 'under', 'with', 'to', \"mightn't\", 'while', \n",
    "'its', 'be', 'll', 'don', 'over', 'again', 'their', 'won', 'too', 'during', \n",
    "'shan', 'herself', 'has', 'or', 'from', 'ours', 'into', 'our', 'above', \n",
    "'wouldn', 'you', 'of', 'so', 't', 'he', 'doing', 'as', 'i', 'can', 'shouldn', \n",
    "'have', 'at', 'other', 'hasn', 'more', 'yourselves', 'y', 'yours', 'very', \n",
    "'themselves', 'which', 'these', 'being', 'both', 'aren', 'did', 'than', 'needn',\n",
    " 'for', 'itself', \"haven't\", 'through', 'weren', 'but', 'once', 'isn', \n",
    " 'ourselves', 'didn', 'not', 'yourself', 'mightn', 'after', 've', 'him', \n",
    " 'whom', \"hasn't\", 'a', 'hadn', \"shouldn't\", \"mustn't\", 'those', 'off', \n",
    " 'each', 'was', \"didn't\", \"you'd\", 'where', 'o', 'further', 'below', \"shan't\", \n",
    " 'myself', 'mustn', 'is', 'been', 'just', 'any', 'out', 'that', 'm', 'such', \n",
    " 'me', 'same', 'hers', 'some', 'had', 'does', 'against', 'should', \"you've\", \n",
    " \"doesn't\", \"you're\", 'them', 'am', 'if', 'who', 'few', 'what', 'there', \n",
    " \"don't\", \"weren't\", \"won't\", 'an', 'all', 're', 'it', 'up', \"hadn't\", \"'ll\"]\n",
    "\n",
    "def get_words(s):\n",
    "    \"\"\" Extract a list of words from a sentence string with punctuation, spaces etc \n",
    "    s = sentence \n",
    "    \"\"\"\n",
    "    # strip punctuation \n",
    "    s = re.sub(r'[^\\w\\s]','',s)\n",
    "    # replace newline \n",
    "    s = s.replace('\\n', ' ')\n",
    "    # get rid of spaces\n",
    "    s = \" \".join(s.split())\n",
    "    return s.split(' ')\n",
    "\n",
    "def unique(iter):\n",
    "    \"removes duplicates from iterable preserving order\"\n",
    "    result = list()\n",
    "    seen = set()\n",
    "    for x in iter:\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            result.append(x)\n",
    "    return result\n",
    "\n",
    "def process_candidates(candidates, target):\n",
    "    \"\"\" words to lower case, replace underscores, remove duplicated words, \n",
    "        filter out target word and stop words \"\"\"\n",
    "    filterwords = stopwords + [target]\n",
    "    return unique(filter(lambda x : x not in filterwords, \n",
    "                  map(lambda s : s.lower().replace('_', ' '), candidates)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, optparse\n",
    "import tqdm\n",
    "import pymagnitude\n",
    "from functools import reduce \n",
    "import operator\n",
    "\n",
    "\n",
    "class LexSub:\n",
    "\n",
    "    def __init__(self, wvec_file, topn=100):\n",
    "        self.wvecs = pymagnitude.Magnitude(wvec_file)\n",
    "        self.n_candidates = 100\n",
    "        self.n_substitutes = 10\n",
    "\n",
    "    def substitutes(self, index, sentence):\n",
    "        \"Return ten guesses that are appropriate lexical substitutions for the word at sentence[index].\"\n",
    "        sentence_list = sentence.split(\" \")\n",
    "        #return(list(map(lambda k: k[0], self.wvecs.most_similar(sentence_list[index], topn=self.topn))))\n",
    "        word = sentence_list[index] \n",
    "        words_scores = self.wvecs.most_similar(positive=[word])\n",
    "        result = [word for word, score in words_scores]\n",
    "        result = process_candidates(result, word)[:self.n_candidates]\n",
    "        return result\n",
    "    \n",
    "    def prod(self, factors):\n",
    "        return reduce(operator.mul, factors, 1)\n",
    "    \n",
    "    \n",
    "    def mult(self, t, s, C):\n",
    "        '''\n",
    "        Mult\n",
    "        '''            \n",
    "        tscore = self.wvecs.similarity(t, s)\n",
    "        ptscore = (tscore + 1)/2\n",
    "        pcscores = [(self.wvecs.similarity(s, c)+1)/2 for c in C ]\n",
    "        pcscore = self.prod(pcscores)\n",
    "        return (pcscore*ptscore)**(1.0/(len(C)+1))\n",
    "\n",
    "    def bal_mult(self, t, s, C):\n",
    "        '''\n",
    "        BalMult\n",
    "        '''\n",
    "        tscore = self.wvecs.similarity(t,s)\n",
    "        ptscore = (tscore + 1)/2\n",
    "        pcscores = [(self.wvecs.similarity(s, c)+1)/2 for c in C ]\n",
    "        pcscore = self.prod(pcscores)\n",
    "        return (((ptscore)**len(C))*pcscore)**(1.0/(2*len(C)))\n",
    "        \n",
    "    def get_substitutability(self, t, s, C):\n",
    "        \"\"\" get substitutability of substitution s for target t in context C\n",
    "        t = target word \n",
    "        s = candidate substitution \n",
    "        C = list of context words \n",
    "        \"\"\"\n",
    "        # 1. target score: how similar is it to the target word?\n",
    "        tscore = self.wvecs.similarity(t, s)\n",
    "        # 2. context score: how similar is it to the context words?\n",
    "        cscores = [self.wvecs.similarity(s, c) for c in C ]\n",
    "        cscore = sum(cscores)\n",
    "        return (len(C)*tscore + cscore)/(2*len(C))\n",
    "\n",
    "    def lex_sub(self, index, sentence):\n",
    "        \"\"\" Get appropriate substitution for a word given context words \n",
    "        \n",
    "        word_POS = word with part of speech in form word.POS e.g. dog.n\n",
    "        context_words = list of words in context \n",
    "        \"\"\"\n",
    "        list_ = sentence.split(\" \")\n",
    "        w = list_[index]\n",
    "        #w,_,POS = word_POS.partition('.')\n",
    "        # generate candidate substitutions\n",
    "        candidates = self.substitutes(index, sentence)\n",
    "        if sentence is None:\n",
    "            return candidates[:self.n_substitutes]\n",
    "        else:\n",
    "            context_words = get_words(sentence)\n",
    "            # filter context words: exist in the word2vec vocab, not stop words  \n",
    "            #context_words = list(filter(lambda c : c in vocabQHat \n",
    "                                              # and c not in stopwords, \n",
    "                                              # context_words))\n",
    "            context_words = list(filter(lambda c : c in self.wvecs \n",
    "                                              and c not in stopwords, \n",
    "                                              context_words))       \n",
    "            cand_scores = [self.get_substitutability(w, s, context_words) if s in self.wvecs else 0 for s in candidates ]\n",
    "            assert(len(cand_scores) == len(candidates))            \n",
    "            sorted_candidates = sorted(zip(candidates, cand_scores), key = lambda x : x[1], reverse=True )\n",
    "            return [sub for sub, score in sorted_candidates][:self.n_substitutes]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "\n",
    "lexsub = LexSub(os.path.join('../','data','glove.6B.100d.retrofit.magnitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english place back way point edge line position front\n",
      "way place position english back line point front edge\n",
      "way english place back point position line edge front\n",
      "way place english back line point position edge front\n",
      "english place way line back point edge position front\n",
      "way english place line back position point edge front\n",
      "place way english back point line edge front position\n",
      "way english place back position line point edge front\n",
      "way english line place position point edge back front\n",
      "edge english place line way point back position front\n",
      "Score=44.16\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "output = []\n",
    "\n",
    "# Run on the retrofit file\n",
    "with open(os.path.join(DATA,'input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        index = int(fields[0])\n",
    "        list_sentence = fields[1].split(\" \")\n",
    "        \n",
    "        output.append(\" \".join(lexsub.lex_sub(int(fields[0]), fields[1])))\n",
    "print(\"\\n\".join(output[:10]))\n",
    "\n",
    "# Check score\n",
    "with open(os.path.join(DATA,'reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
